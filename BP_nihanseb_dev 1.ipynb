{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyclesperanto_prototype as cle\n",
    "import pandas as pd\n",
    "from skimage.filters import gaussian, threshold_otsu\n",
    "from skimage import measure\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "import plotly.express as px\n",
    "from utils_stardist import get_gpu_details, list_images, read_image, maximum_intensity_projection\n",
    "\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the path where your images are stored, you can use absolute or relative paths to point at other disk locations\n",
    "directory_path = Path(\"../raw_data/nihanseb_organoid\")\n",
    "\n",
    "# Define the nuclei and markers of interest channel order ('Remember in Python one starts counting from zero')\n",
    "nuclei_channel = 2\n",
    "\n",
    "# Image size reduction (downsampling) to improve processing times (slicing, not lossless compression)\n",
    "# Now, in addition to xy, you can downsample across your z-stack\n",
    "slicing_factor_xy = 4 # Use 2 or 4 for downsampling in xy (None for lossless)\n",
    "slicing_factor_z = None # Use 2 to select 1 out of every 2 z-slices\n",
    "\n",
    "# Define the channels you want to analyze using the following structure:\n",
    "# markers = [(channel_name, channel_nr),(..., ...)]\n",
    "# Remember in Python one starts counting from 0, so your first channel will be 0\n",
    "# i.e. markers = [(\"ARSA\", 0), (\"MBP\", 1)]\n",
    "markers = [(\"ARSA\", 0), (\"MBP\", 1)]\n",
    "\n",
    "# Fill holes inside the resulting organoid mask? Set to False if you want to keep the holes\n",
    "fill_holes = True\n",
    "\n",
    "# Dilate your labels (set value to add 1, 2, 3 pixels around them)\n",
    "dilation_radius = 0\n",
    "\n",
    "# Iterate through the .czi and .nd2 files in the raw_data directory\n",
    "images = list_images(directory_path)\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the experiment name from the data directory path\n",
    "experiment_id = directory_path.name\n",
    "\n",
    "# Create a 'results' folder in the root directory\n",
    "results_folder = Path(\"results\") / experiment_id\n",
    "\n",
    "try:\n",
    "    os.makedirs(results_folder)\n",
    "    print(f\"'{results_folder}' folder created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"'{results_folder}' folder already exists.\")\n",
    "\n",
    "for image in tqdm (images):\n",
    "\n",
    "    # Read image, apply slicing if needed and return filename and img as a np array\n",
    "    img, filename = read_image(image, slicing_factor_xy, slicing_factor_z)\n",
    "\n",
    "    # Generate maximum intensity projection \n",
    "    img_mip = maximum_intensity_projection(img)\n",
    "\n",
    "    # Generate mean intensity projection\n",
    "    img_mean = np.mean(img, axis=1)\n",
    "\n",
    "    # Extract nuclei channel to create an organoid mask based on this\n",
    "    nuclei_img_mip = img_mip[nuclei_channel]\n",
    "\n",
    "    # Blur nuclei to \"fuse\" them\n",
    "    blurred_nuclei = gaussian(nuclei_img_mip, sigma=10)\n",
    "\n",
    "    # Extract a threshold separating background from foreground (nuclei) using Otsu and generate a mask\n",
    "    organoid_mask = blurred_nuclei > threshold_otsu(blurred_nuclei)\n",
    "\n",
    "    if fill_holes:\n",
    "\n",
    "        # Close empty holes surrounded by True pixels\n",
    "        organoid_mask = binary_fill_holes(organoid_mask)\n",
    "\n",
    "    # Label connected components to filter out small ones later on\n",
    "    organoid_labels = measure.label(organoid_mask)\n",
    "\n",
    "    # Dilate labels to cover surrounding areas\n",
    "    organoid_labels = cle.dilate_labels(organoid_labels, radius=dilation_radius)\n",
    "    organoid_labels = cle.pull(organoid_labels)\n",
    "\n",
    "    # Initialize an empty list to hold the extracted dataframes on a per channel basis\n",
    "    props_list = []\n",
    "\n",
    "    # Create a dictionary containing all image descriptors\n",
    "    descriptor_dict = {\n",
    "                \"filename\": filename,\n",
    "                \"fill_holes\": fill_holes,\n",
    "                \"dilation_radius\":dilation_radius,\n",
    "                \"slicing_factor_xy\": slicing_factor_xy\n",
    "                }\n",
    "\n",
    "    for channel_name, ch_nr in tqdm(markers):\n",
    "\n",
    "        # Extract intensity information from each marker channel\n",
    "        props = measure.regionprops_table(label_image=organoid_labels,\n",
    "                                intensity_image=img_mip[ch_nr],\n",
    "                                properties=[\"label\", \"area\", \"intensity_mean\"])\n",
    "        \n",
    "        # Convert to dataframe\n",
    "        props_df = pd.DataFrame(props)\n",
    "\n",
    "        # Rename intensity_mean column to indicate the specific image\n",
    "        props_df.rename(columns={\"intensity_mean\": f\"{channel_name}_avg_int\"}, inplace=True)\n",
    "\n",
    "        # Append each props_df to props_list\n",
    "        props_list.append(props_df)\n",
    "\n",
    "    # Initialize the df with the first df in the list\n",
    "    props_df = props_list[0]\n",
    "    # Start looping from the second df in the list\n",
    "    for df in props_list[1:]:\n",
    "        props_df = props_df.merge(df, on=(\"label\",\"area\"))\n",
    "\n",
    "    # Add each key-value pair from descriptor_dict to props_df at the specified position\n",
    "    insertion_position = 0    \n",
    "    for key, value in descriptor_dict.items():\n",
    "        props_df.insert(insertion_position, key, value)\n",
    "        insertion_position += 1  # Increment position to maintain the order of keys in descriptor_dict\n",
    "\n",
    "    # Sort by area in descending order\n",
    "    props_df = props_df.sort_values(by='area', ascending=False)\n",
    "\n",
    "    # Save the df containing per_label results into a CSV file\n",
    "    props_df.to_csv(results_folder / f'{filename}_per_label_avg_int.csv')\n",
    "\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(results_folder, \"*.csv\"))\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "all_dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "output_path = os.path.join(results_folder, \"BP_per_filename_summary.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"All CSV files concatenated and saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_nuc_stardist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
