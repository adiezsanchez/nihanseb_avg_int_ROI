{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: /device:GPU:0\n",
      "Device type: GPU\n",
      "GPU model: device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import nd2\n",
    "import tifffile\n",
    "import napari\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyclesperanto_prototype as cle\n",
    "import pandas as pd\n",
    "from skimage import exposure\n",
    "from skimage import measure\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "import plotly.express as px\n",
    "from utils import get_gpu_details, list_images, read_image, maximum_intensity_projection\n",
    "\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_data\\\\nihanseb_organoid\\\\MLD 1.8 block4 ARSA MBP batch 1 40x.nd2',\n",
       " 'raw_data\\\\nihanseb_organoid\\\\MLD 2.2 block7 MBP MAP2 slide 7 batch 2 40x.nd2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the path where your images are stored, you can use absolute or relative paths to point at other disk locations\n",
    "directory_path = Path(\"./raw_data/nihanseb_organoid\")\n",
    "\n",
    "# Define the nuclei and markers of interest channel order ('Remember in Python one starts counting from zero')\n",
    "nuclei_channel = 2\n",
    "\n",
    "# Define the channels you want to analyze using the following structure:\n",
    "# markers = [(channel_name, channel_nr),(..., ...)]\n",
    "# Remember in Python one starts counting from 0, so your first channel will be 0\n",
    "# i.e. markers = [(\"ARSA\", 0), (\"MBP\", 1)]\n",
    "markers = [(\"ARSA\", 0), (\"MBP\", 1)]\n",
    "\n",
    "# Fill holes inside the resulting organoid mask? Set to False if you want to keep the holes\n",
    "fill_holes = True\n",
    "\n",
    "# Analyze intensity within the 3D volume of the ROI, or perform a mean or max intensity projection of the marker channel (2D)\n",
    "analysis_type = \"2D\" #\"2D\" or \"3D\"\n",
    "\n",
    "# If 2D analysis type, Choose projection type (mean intensity or max intensity)\n",
    "projection_type = \"mean\" # \"mean\" or \"max\"\n",
    "\n",
    "# Stardist model name if nuclei labels predictions are present\n",
    "model_name = None\n",
    "\n",
    "# Iterate through the .czi and .nd2 files in the raw_data directory\n",
    "images = list_images(directory_path)\n",
    "\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Image analyzed: MLD 1.8 block4 ARSA MBP batch 1 40x\n",
      "Original Array shape: (3, 24, 10797, 10797)\n",
      "Compressed Array shape: (3, 24, 2700, 2700)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'mean_projection' at 0x25523c0faf0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore each image to analyze (0 defines the first image in the directory)\n",
    "image = images[0]\n",
    "\n",
    "# Image size reduction (downsampling) to improve processing times (slicing, not lossless compression)\n",
    "# Now, in addition to xy, you can downsample across your z-stack\n",
    "slicing_factor_xy = 4 # Use 2 or 4 for downsampling in xy (None for lossless)\n",
    "slicing_factor_z = None # Use 2 to select 1 out of every 2 z-slices\n",
    "\n",
    "# Read image, apply slicing if needed and return filename and img as a np array\n",
    "img, filename = read_image(image, slicing_factor_xy, slicing_factor_z)\n",
    "\n",
    "# Generate maximum or mean intensity projection\n",
    "if projection_type == \"max\":\n",
    "    img_projection = np.max(img, axis=1)\n",
    "elif projection_type == \"mean\":\n",
    "    img_projection = np.mean(img, axis=1)\n",
    "\n",
    "# Show image in Napari\n",
    "viewer = napari.Viewer(ndisplay=2)\n",
    "viewer.add_image(img_projection, name=f\"{projection_type}_projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'organoid_mask [1]' at 0x255252dbd00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct ROI and nuclei predictions paths from directory_path above\n",
    "roi_path = directory_path / \"ROIs\"\n",
    "# nuclei_preds_path =  directory_path / \"nuclei_preds\" / analysis_type / model_name\n",
    "\n",
    "# Extract the experiment name from the data directory path\n",
    "experiment_id = directory_path.name\n",
    "\n",
    "# Check for presence of ROIs\n",
    "try:\n",
    "    roi_names = [folder.name for folder in roi_path.iterdir() if folder.is_dir()]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    roi_names = [\"auto_generated_ROI\"]\n",
    "    print(\"No manually defined ROI found, generating ROI automatically...\")\n",
    "\n",
    "# Read previously defined ROIs\n",
    "organoid_mask = tifffile.imread(roi_path / roi_names[0] / f\"{filename}.tiff\")\n",
    "viewer.add_labels(organoid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the slicing factor\n",
    "roi_slicing_factor = round(organoid_mask.shape[-1] / img.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10797"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "organoid_mask.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following regions of interest will be analyzed: ['organoid']\n",
      "'results\\nihanseb_organoid' folder already exists.\n",
      "\n",
      "Analyzing ROI: organoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Construct ROI and nuclei predictions paths from directory_path above\n",
    "roi_path = directory_path / \"ROIs\"\n",
    "# nuclei_preds_path =  directory_path / \"nuclei_preds\" / analysis_type / model_name\n",
    "\n",
    "# Extract the experiment name from the data directory path\n",
    "experiment_id = directory_path.name\n",
    "\n",
    "# Check for presence of ROIs\n",
    "try:\n",
    "    roi_names = [folder.name for folder in roi_path.iterdir() if folder.is_dir()]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    roi_names = [\"auto_generated_ROI\"]\n",
    "    print(\"No manually defined ROI found, generating ROI automatically...\")\n",
    "        \n",
    "print(f\"The following regions of interest will be analyzed: {roi_names}\")\n",
    "\n",
    "# Create a 'results' folder in the root directory to store results\n",
    "results_folder = Path(\"results\") / experiment_id\n",
    "\n",
    "try:\n",
    "    os.makedirs(results_folder)\n",
    "    print(f\"'{results_folder}' folder created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"'{results_folder}' folder already exists.\")\n",
    "\n",
    "for roi_name in roi_names:\n",
    "\n",
    "        print(f\"\\nAnalyzing ROI: {roi_name}\")\n",
    "\n",
    "        # Initialize an empty list to hold the extracted dataframes on a per channel basis\n",
    "        props_list = []\n",
    "\n",
    "        # Read the user defined ROIs, in case of full image analysis generate a label covering the entire image\n",
    "        try:\n",
    "            # Read previously defined ROIs\n",
    "            organoid_mask = tifffile.imread(roi_path / roi_name / f\"{filename}.tiff\")\n",
    "            viewer.add_labels(organoid_mask)\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            # Add logic to automatically generate an organoid mask\n",
    "            pass\n",
    "\n",
    "        #TODO: Check if ROI shape and input image match (if not apply slicing factor)\n",
    "        \n",
    "\n",
    "        # If analysis type == \"3D\" extend ROI over the entire volume\n",
    "        if analysis_type == \"3D\":\n",
    "            # Extract the number of z-slices to extend the mask\n",
    "            slice_nr = img.shape[1]\n",
    "            # Extend the mask across the entire volume\n",
    "            organoid_mask = np.tile(organoid_mask, (slice_nr, 1, 1))\n",
    "\n",
    "            viewer.add_labels(organoid_mask)\n",
    "\n",
    "        if fill_holes:\n",
    "\n",
    "            # Close empty holes surrounded by True pixels\n",
    "            organoid_mask = binary_fill_holes(organoid_mask)\n",
    "            viewer.add_labels(organoid_mask, name=\"closed_organoids_mask\")\n",
    "\n",
    "        # Transform organoid mask into a label type without the need to perform connected components\n",
    "        organoid_mask = organoid_mask.astype(np.uint8)\n",
    "\n",
    "        # Initialize an empty list to hold the extracted dataframes on a per channel basis\n",
    "        props_list = []\n",
    "\n",
    "        # Create a dictionary containing all image descriptors\n",
    "        descriptor_dict = {\n",
    "                    \"filename\": filename,\n",
    "                    \"fill_holes\": fill_holes,\n",
    "                    \"slicing_factor_xy\": slicing_factor_xy\n",
    "                    }\n",
    "\n",
    "        for channel_name, ch_nr in tqdm(markers):\n",
    "\n",
    "            # Extract intensity information from each marker channel\n",
    "            props = measure.regionprops_table(label_image=organoid_mask,\n",
    "                                    intensity_image=img_projection[ch_nr],\n",
    "                                    properties=[\"label\", \"area\", \"intensity_mean\"])\n",
    "            \n",
    "            # Convert to dataframe\n",
    "            props_df = pd.DataFrame(props)\n",
    "\n",
    "            # Rename intensity_mean column to indicate the specific image\n",
    "            props_df.rename(columns={\"intensity_mean\": f\"{channel_name}_avg_int\"}, inplace=True)\n",
    "\n",
    "            # Append each props_df to props_list\n",
    "            props_list.append(props_df)\n",
    "\n",
    "        # Initialize the df with the first df in the list\n",
    "        props_df = props_list[0]\n",
    "        # Start looping from the second df in the list\n",
    "        for df in props_list[1:]:\n",
    "            props_df = props_df.merge(df, on=(\"label\",\"area\"))\n",
    "\n",
    "        # Add each key-value pair from descriptor_dict to props_df at the specified position\n",
    "        insertion_position = 0    \n",
    "        for key, value in descriptor_dict.items():\n",
    "            props_df.insert(insertion_position, key, value)\n",
    "            insertion_position += 1  # Increment position to maintain the order of keys in descriptor_dict\n",
    "\n",
    "        # Sort by area in descending order\n",
    "        props_df = props_df.sort_values(by='area', ascending=False)\n",
    "\n",
    "        # Save the df containing per_label results into a CSV file\n",
    "        props_df.to_csv(results_folder / f'{filename}_per_label_avg_int.csv')\n",
    "\n",
    "        props_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel size: 0.166 µm x 0.166 µm\n",
      "Voxel (Z-step) size: 1.000 µm\n"
     ]
    }
   ],
   "source": [
    "with nd2.ND2File(image) as nd2_data:\n",
    "    # Get the first channel's volume metadata\n",
    "    first_channel = nd2_data.metadata.channels[0]\n",
    "    voxel_size = first_channel.volume.axesCalibration  # X, Y, Z calibration\n",
    "\n",
    "    # Extract pixel sizes\n",
    "    pixel_size_x, pixel_size_y, voxel_size_z = voxel_size\n",
    "\n",
    "    print(f\"Pixel size: {pixel_size_x:.3f} µm x {pixel_size_y:.3f} µm\")\n",
    "    print(f\"Voxel (Z-step) size: {voxel_size_z:.3f} µm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_nuc_stardist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
